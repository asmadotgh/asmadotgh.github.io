<!doctype html> <html lang="en"> <head> <meta name="google-site-verification" content="YtalD1nfTxeU3Dwd5XhnYxtuBvaCKZK0hSSaRg2aZOQ"> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Asma Ghandeharioun </title> <meta name="author" content="Asma Ghandeharioun"> <meta name="description" content="Asma Ghandeharioun's personal website. "> <meta name="keywords" content="interpretability, alignment, mechanistic interpretability, language models, Tranformers"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¬</text></svg>" > <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://asmadotgh.github.io/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%61%67%68%61%6E%64%65%68%61%72%69%6F%75%6E@%67%6F%6F%67%6C%65.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=CkfQy2gAAAAJ" title="Google Scholar"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/asmadotgh" title="GitHub"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/asma-ghandeharioun-4229897" title="LinkedIn"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/ghandeharioun" title="X"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">Bio </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Asma</span> Ghandeharioun </h1> <p class="desc">Senior Research Scientist, Google DeepMind, NYC</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw"" type="image/webp"> <img src="/assets/img/prof_pic.jpg?46b445ddd2899e05e7296233be4a8801" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"><p>I am Asma Ghandeharioun, a senior research scientist in Google DeepMind. I work on aligning AI with human values through better understanding [<a href="https://pair-code.github.io/interpretability/patchscopes/">1</a>] and controlling (language) models [<a href="/assets/pdf/2928_post_hoc_explanations_of_langu.pdf">2</a>], and uniquely by demystifying their inner workings [<a href="https://pair.withgoogle.com/explorables/grokking/">3</a>] and correcting collective misconceptions along the way [<a href="/assets/pdf/13353_does_localization_inform_editi.pdf">4</a>, <a href="https://arxiv.org/abs/2312.03656">5</a>].</p> <p>I received my <a href="/assets/pdf/ghandeharioun-asma_gh-phd-MAS-2021-thesis.pdf">Ph.D.</a> from the <a href="https://www.media.mit.edu/groups/affective-computing/overview/">Affective Computing Group</a>, <a href="https://www.media.mit.edu/">MIT Media Lab</a>. I am fortunate to have had <a href="https://www.media.mit.edu/people/picard/overview/">Roz</a> as my advisor. In addition, I have had research experiences at Google Research, Microsoft Research, and EPFL, many of which have evolved into exciting long-term collaborations.</p> <p>In a previous life, I conducted research in the digital mental health and wellbeing space, collaborated with medical professionals from Harvard and renowned hospitals in the Boston area such as Massachusetts General Hospital (MGH) and Brigham and Womenâ€™s Hospital (BWH), and published in venues such as Frontiers in Psychiatry and Psychology of Well-being. This area remains close to my heart, and I occasionally dabble in it during my free time.</p> <p style="font-weight: bold;"><a href="/assets/pdf/Asma_Ghandeharioun_resume_2024.pdf">You can download my rÃ©sumÃ© here.</a></p> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/patchscopes-480.webp 480w,/assets/img/publication_preview/patchscopes-800.webp 800w,/assets/img/publication_preview/patchscopes-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/patchscopes.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="patchscopes.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ghandeharioun2024patchscopes" class="col-sm-8"> <div class="title">Patchscopes: A unifying framework for inspecting hidden representations of language models</div> <div class="author"> <em>Asma Ghandeharioun*</em>,&nbsp;Avi Caciularu* ,&nbsp;Adam Pearce ,&nbsp;Lucas Dixon ,&nbsp;and&nbsp;Mor Geva </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML), to appear</em>, 2024 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2401.06102" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/PAIR-code/interpretability/tree/master/patchscopes/code" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="https://pair-code.github.io/interpretability/patchscopes" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Understanding the internal representations of large language models (LLMs) can help explain modelsâ€™ behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLMâ€™s computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and self-correction in multihop reasoning, even outperforming chain-of-thought prompting.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ghandeharioun2024patchscopes</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Patchscopes: A unifying framework for inspecting hidden representations of language models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghandeharioun*, Asma and Caciularu*, Avi and Pearce, Adam and Dixon, Lucas and Geva, Mor}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning (ICML), to appear}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/localization-480.webp 480w,/assets/img/publication_preview/localization-800.webp 800w,/assets/img/publication_preview/localization-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/localization.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="localization.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hase2024does" class="col-sm-8"> <div class="title">Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models</div> <div class="author"> Peter Hase ,&nbsp;Mohit Bansal ,&nbsp;Been Kim ,&nbsp;and&nbsp;<em>Asma Ghandeharioun</em> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> (Spotlight) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/13353_does_localization_inform_editi.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/google/belief-localization" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Language models learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights. In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific model parameters would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit. Next, we consider several variants of the editing problem, including erasing and amplifying facts. For one of our editing problems, editing performance does relate to localization results from representation denoising, but we find that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hase2024does</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{36}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/simplification-480.webp 480w,/assets/img/publication_preview/simplification-800.webp 800w,/assets/img/publication_preview/simplification-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/simplification.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="simplification.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="friedman2023interpretability" class="col-sm-8"> <div class="title">Interpretability illusions in the generalization of simplified models</div> <div class="author"> Dan Friedman ,&nbsp;Andrew Kyle Lampinen ,&nbsp;Lucas Dixon ,&nbsp;Danqi Chen ,&nbsp;and&nbsp;<em>Asma Ghandeharioun</em> </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML), to appear</em>, 2024 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2312.03656" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>A common method to study deep learning systems is to create simplified representationsâ€”for example, using singular value decomposition to visualize the modelâ€™s hidden states in a lower dimensional space. This approach assumes that the simplified model is faithful to the original model. Here, we illustrate an important caveat to this assumption: even if a simplified representation of the model can accurately approximate the original model on the training set, it may fail to match its behavior out of distribution; the understanding developed from simplified representations may be an illusion. We illustrate this by training Transformer models on controlled datasets with systematic generalization splits, focusing on the Dyck balanced-parenthesis languages. We simplify these models using tools like dimensionality-reduction and clustering, and find clear patterns in the resulting representations. We then explicitly test how these simplified proxy models match the original models behavior on various out-of-distribution test sets. Generally, the simplified proxies are less faithful out of distribution. For example, in cases where the original model generalizes to novel structures or deeper depths, the simplified model may fail to generalize, or may generalize too well. We then show the generality of these results: even model simplifications that do not directly use data can be less faithful out of distribution, and other tasks can also yield generalization gaps. Our experiments raise questions about the extent to which mechanistic interpretations derived using tools like SVD can reliably predict what a model will do in novel situations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">friedman2023interpretability</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Interpretability illusions in the generalization of simplified models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Friedman, Dan and Lampinen, Andrew Kyle and Dixon, Lucas and Chen, Danqi and Ghandeharioun, Asma}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning (ICML), to appear}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/grok-480.webp 480w,/assets/img/publication_preview/grok-800.webp 800w,/assets/img/publication_preview/grok-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/grok.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="grok.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pearce2023machine" class="col-sm-8"> <div class="title">Do machine learning models memorize or generalize</div> <div class="author"> Adam Pearce ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Nada Hussein ,&nbsp;Nithum Thain ,&nbsp;Martin Wattenberg ,&nbsp;and&nbsp;Lucas Dixon </div> <div class="periodical"> <em>In IEEE VISxAI</em> , 2023 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> (Best paper) </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/PAIR-code/tiny-transformers" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="https://pair.withgoogle.com/explorables/grokking/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pearce2023machine</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Do machine learning models memorize or generalize}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pearce, Adam and Ghandeharioun, Asma and Hussein, Nada and Thain, Nithum and Wattenberg, Martin and Dixon, Lucas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE VISxAI}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/AMPLIFY-480.webp 480w,/assets/img/publication_preview/AMPLIFY-800.webp 800w,/assets/img/publication_preview/AMPLIFY-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/AMPLIFY.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="AMPLIFY.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="krishna2023post" class="col-sm-8"> <div class="title">Post Hoc Explanations of Language Models Can Improve Language Models</div> <div class="author"> Satyapriya Krishna ,&nbsp;Jiaqi Ma ,&nbsp;Dylan Slack ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Sameer Singh ,&nbsp;and&nbsp;Himabindu Lakkaraju </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em> , 2023 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2928_post_hoc_explanations_of_langu.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, lead to critical insights for refining in context learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">krishna2023post</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Post Hoc Explanations of Language Models Can Improve Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Krishna, Satyapriya and Ma, Jiaqi and Slack, Dylan and Ghandeharioun, Asma and Singh, Sameer and Lakkaraju, Himabindu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dissect-480.webp 480w,/assets/img/publication_preview/dissect-800.webp 800w,/assets/img/publication_preview/dissect-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/dissect.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dissect.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ghandeharioun2021dissect" class="col-sm-8"> <div class="title">DISSECT: Disentangled simultaneous explanations via concept traversals</div> <div class="author"> <em>Asma Ghandeharioun</em>,&nbsp;Been Kim ,&nbsp;Chun-Liang Li ,&nbsp;Brendan Jou ,&nbsp;Brian Eoff ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR)</em> , 2021 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://mitmedialabaffectivecomputing.github.io/SynthDermDataset/" class="btn btn-sm z-depth-0" role="button">DATA</a> <a href="/assets/pdf/4164_dissect_disentangled_simultane.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/asmadotgh/dissect" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore "what-if" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifierâ€™s decision. By training a generative model from a classifierâ€™s signal, DISSECT offers a way to discover a classifierâ€™s inherent "notion" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifierâ€™s decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ghandeharioun2021dissect</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DISSECT: Disentangled simultaneous explanations via concept traversals}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghandeharioun, Asma and Kim, Been and Li, Chun-Liang and Jou, Brendan and Eoff, Brian and Picard, Rosalind W}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/correlations_table-480.webp 480w,/assets/img/publication_preview/correlations_table-800.webp 800w,/assets/img/publication_preview/correlations_table-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/correlations_table.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="correlations_table.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ghandeharioun2019approximating" class="col-sm-8"> <div class="title">Approximating interactive human evaluation with self-play for open-domain dialog systems</div> <div class="author"> <em>Asma Ghandeharioun*</em>,&nbsp;Judy Hanwen Shen* ,&nbsp;Natasha Jaques* ,&nbsp;Craig Ferguson ,&nbsp;Noah Jones ,&nbsp;Agata Lapedriza ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em> , 2019 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/NeurIPS-2019-approximating-interactive-human-evaluation-with-self-play-for-open-domain-dialog-systems-Paper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/natashamjaques/neural_chat" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Building an open-domain conversational agent is a challenging problem. Current evaluation methods, mostly post-hoc judgments of static conversation, do not capture conversation quality in a realistic interactive context. In this paper, we investigate interactive human evaluation and provide evidence for its necessity; we then introduce a novel, model-agnostic, and dataset-agnostic method to approximate it. In particular, we propose a self-play scenario where the dialog system talks to itself and we calculate a combination of proxies such as sentiment and semantic coherence on the conversation trajectory. We show that this metric is capable of capturing the human-rated quality of a dialog model better than any automated metric known to-date, achieving a significant Pearson correlation (r>.7, p<.05). To investigate the strengths of this novel metric and interactive evaluation in comparison to state-of-the-art metrics and human evaluation of static conversations, we perform extended experiments with a set of models, including several that make novel improvements to recent hierarchical dialog generation architectures through sentiment and semantic knowledge distillation on the utterance level. Finally, we open-source the interactive evaluation platform we built and the dataset we collected to allow researchers to efficiently deploy and evaluate dialog models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ghandeharioun2019approximating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Approximating interactive human evaluation with self-play for open-domain dialog systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghandeharioun*, Asma and Shen*, Judy Hanwen and Jaques*, Natasha and Ferguson, Craig and Jones, Noah and Lapedriza, Agata and Picard, Rosalind W}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/thesis-480.webp 480w,/assets/img/publication_preview/thesis-800.webp 800w,/assets/img/publication_preview/thesis-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/thesis.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="thesis.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ghandeharioun2021towards" class="col-sm-8"> <div class="title">Towards Human-Centered Optimality Criteria</div> <div class="author"> <em>Asma Ghandeharioun</em> </div> <div class="periodical"> <em>Massachusetts Institute of Technology</em> , 2021 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/ghandeharioun-asma_gh-phd-MAS-2021-thesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Despite the transformational success of machine learning across various applications, examples of deployed models failing to recognize and support human-centered (HC) criteria are abundant. In this thesis, I conceptualize the space of human-machine collaboration with respect to two components: interpretation of people by machines and interpretation of machines by people. I develop several tools that make improvements along these axes. First, I develop a pipeline that predicts depressive symptoms rated by clinicians from real-world longitudinal data outperforming several baselines. Second, I introduce a novel, model-agnostic, and dataset-agnostic method to approximate interactive human evaluation in open-domain dialog through self-play that is more strongly correlated with human evaluations than other automated metrics commonly used today. While dialog quality evaluation metrics predominantly use word-level overlap or distance metrics based on embedding resemblance to each turn of the conversation, I show the significance of taking into account the conversationâ€™s trajectory and using proxies such as sentiment, semantics, and user engagement that are psychologically motivated. Third, I demonstrate an uncertainty measurement technique that helps disambiguate annotator disagreement and data bias. I show that this characterization also improves model performance. Finally, I present a novel method that allows humans to investigate a predictorâ€™s decision-making process to gain better insight into how it works. The method jointly trains a generator, a discriminator, and a concept disentangler, allowing the human to ask "what-if" questions. I evaluate it on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that our method performs consistently well across all. I discuss its applications to detect potential biases of a classifier and identify spurious artifacts that impact predictions using simulated experiments. Together, these novel techniques and insights provide a more comprehensive interpretation of people by machines and more powerful tools for interpretation of machines by people that can move us closer to HC optimality.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">ghandeharioun2021towards</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Human-Centered Optimality Criteria}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghandeharioun, Asma}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Massachusetts Institute of Technology}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%61%67%68%61%6E%64%65%68%61%72%69%6F%75%6E@%67%6F%6F%67%6C%65.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=CkfQy2gAAAAJ" title="Google Scholar"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/asmadotgh" title="GitHub"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/asma-ghandeharioun-4229897" title="LinkedIn"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/ghandeharioun" title="X"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> &copy; Copyright 2024 Asma Ghandeharioun. Last updated: July 11, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-FNVJMQM77L"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-FNVJMQM77L");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>