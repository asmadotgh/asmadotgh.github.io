<!doctype html> <html lang="en"> <head> <meta name="google-site-verification" content="YtalD1nfTxeU3Dwd5XhnYxtuBvaCKZK0hSSaRg2aZOQ"> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Asma Ghandeharioun </title> <meta name="author" content="Asma Ghandeharioun"> <meta name="description" content="Asma Ghandeharioun's personal website. "> <meta name="keywords" content="interpretability, alignment, mechanistic interpretability, language models, Tranformers"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🔬</text></svg>" > <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://asmadotgh.github.io/publications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Asma</span> Ghandeharioun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">Bio </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <p>For a more complete list, see <a href="https://scholar.google.com/citations?user=CkfQy2gAAAAJ&amp;hl=en"> Google scholar</a>.</p> <p style="font-style: italic;">*Equal contribution.</p> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/patchscopes-480.webp 480w,/assets/img/publication_preview/patchscopes-800.webp 800w,/assets/img/publication_preview/patchscopes-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/patchscopes.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="patchscopes.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2024patchscopes" class="col-sm-8"> <div class="title">Patchscopes: A unifying framework for inspecting hidden representations of language models</div> <div class="author"> <em>Asma Ghandeharioun*</em>,&nbsp;Avi Caciularu* ,&nbsp;Adam Pearce ,&nbsp;Lucas Dixon ,&nbsp;and&nbsp;Mor Geva </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML), to appear</em>, 2024 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2401.06102" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/PAIR-code/interpretability/tree/master/patchscopes/code" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="https://pair-code.github.io/interpretability/patchscopes" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Understanding the internal representations of large language models (LLMs) can help explain models’ behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM’s computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and self-correction in multihop reasoning, even outperforming chain-of-thought prompting.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ghandeharioun2024patchscopes</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Patchscopes: A unifying framework for inspecting hidden representations of language models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghandeharioun*, Asma and Caciularu*, Avi and Pearce, Adam and Dixon, Lucas and Geva, Mor}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning (ICML), to appear}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/simplification-480.webp 480w,/assets/img/publication_preview/simplification-800.webp 800w,/assets/img/publication_preview/simplification-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/simplification.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="simplification.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="friedman2023interpretability" class="col-sm-8"> <div class="title">Interpretability illusions in the generalization of simplified models</div> <div class="author"> Dan Friedman ,&nbsp;Andrew Kyle Lampinen ,&nbsp;Lucas Dixon ,&nbsp;Danqi Chen ,&nbsp;and&nbsp;<em>Asma Ghandeharioun</em> </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML), to appear</em>, 2024 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2312.03656" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>A common method to study deep learning systems is to create simplified representations—for example, using singular value decomposition to visualize the model’s hidden states in a lower dimensional space. This approach assumes that the simplified model is faithful to the original model. Here, we illustrate an important caveat to this assumption: even if a simplified representation of the model can accurately approximate the original model on the training set, it may fail to match its behavior out of distribution; the understanding developed from simplified representations may be an illusion. We illustrate this by training Transformer models on controlled datasets with systematic generalization splits, focusing on the Dyck balanced-parenthesis languages. We simplify these models using tools like dimensionality-reduction and clustering, and find clear patterns in the resulting representations. We then explicitly test how these simplified proxy models match the original models behavior on various out-of-distribution test sets. Generally, the simplified proxies are less faithful out of distribution. For example, in cases where the original model generalizes to novel structures or deeper depths, the simplified model may fail to generalize, or may generalize too well. We then show the generality of these results: even model simplifications that do not directly use data can be less faithful out of distribution, and other tasks can also yield generalization gaps. Our experiments raise questions about the extent to which mechanistic interpretations derived using tools like SVD can reliably predict what a model will do in novel situations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">friedman2023interpretability</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Interpretability illusions in the generalization of simplified models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Friedman, Dan and Lampinen, Andrew Kyle and Dixon, Lucas and Chen, Danqi and Ghandeharioun, Asma}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning (ICML), to appear}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/localization-480.webp 480w,/assets/img/publication_preview/localization-800.webp 800w,/assets/img/publication_preview/localization-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/localization.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="localization.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="hase2024does" class="col-sm-8"> <div class="title">Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models</div> <div class="author"> Peter Hase ,&nbsp;Mohit Bansal ,&nbsp;Been Kim ,&nbsp;and&nbsp;<em>Asma Ghandeharioun</em> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> (Spotlight) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/13353_does_localization_inform_editi.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/google/belief-localization" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Language models learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights. In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific model parameters would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit. Next, we consider several variants of the editing problem, including erasing and amplifying facts. For one of our editing problems, editing performance does relate to localization results from representation denoising, but we find that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hase2024does</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{36}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/repvfunc-480.webp 480w,/assets/img/publication_preview/repvfunc-800.webp 800w,/assets/img/publication_preview/repvfunc-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/repvfunc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="repvfunc.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="friedman2023comparing" class="col-sm-8"> <div class="title">Comparing Representational and Functional Similarity in Small Transformer Language Models</div> <div class="author"> Dan Friedman ,&nbsp;Andrew Kyle Lampinen ,&nbsp;Lucas Dixon ,&nbsp;Danqi Chen ,&nbsp;and&nbsp;<em>Asma Ghandeharioun</em> </div> <div class="periodical"> <em>In UniReps: the First Workshop on Unifying Representations in Neural Models. (NeurIPS-W)</em> , 2023 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> (Oral) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/66_comparing_representational_and.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In many situations, it would be helpful to be able to characterize the solution learned by a neural network, including for answering scientific questions (e.g. how do architecture changes affect generalization) and addressing practical concerns (e.g. auditing for potentially unsafe behavior). One approach is to try to understand these models by studying the representations that they learn—for example, comparing whether two networks learn similar representations. However, it is not always clear how much representation-level analyses can tell us about how a model makes predictions. In this work, we explore this question in the context of small Transformer language models, which we train on a synthetic, hierarchical language task. We train models with different sizes and random initializations, evaluating performance over the course of training and on a variety of systematic generalization splits. We find that existing methods for measuring representation similarity are not always correlated with behavioral metrics—i.e. models with similar representations do not always make similar predictions—and the results vary depending on the choice of representation. Our results highlight the importance of understanding representations in terms of the role they play in the neural algorithm.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">friedman2023comparing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Comparing Representational and Functional Similarity in Small Transformer Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Friedman, Dan and Lampinen, Andrew Kyle and Dixon, Lucas and Chen, Danqi and Ghandeharioun, Asma}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{UniReps: the First Workshop on Unifying Representations in Neural Models. (NeurIPS-W)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/grok-480.webp 480w,/assets/img/publication_preview/grok-800.webp 800w,/assets/img/publication_preview/grok-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/grok.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="grok.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="pearce2023machine" class="col-sm-8"> <div class="title">Do machine learning models memorize or generalize</div> <div class="author"> Adam Pearce ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Nada Hussein ,&nbsp;Nithum Thain ,&nbsp;Martin Wattenberg ,&nbsp;and&nbsp;Lucas Dixon </div> <div class="periodical"> <em>In IEEE VISxAI</em> , 2023 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> (Best paper) </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/PAIR-code/tiny-transformers" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="https://pair.withgoogle.com/explorables/grokking/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pearce2023machine</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Do machine learning models memorize or generalize}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pearce, Adam and Ghandeharioun, Asma and Hussein, Nada and Thain, Nithum and Wattenberg, Martin and Dixon, Lucas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE VISxAI}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/AMPLIFY-480.webp 480w,/assets/img/publication_preview/AMPLIFY-800.webp 800w,/assets/img/publication_preview/AMPLIFY-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/AMPLIFY.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="AMPLIFY.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="krishna2023post" class="col-sm-8"> <div class="title">Post Hoc Explanations of Language Models Can Improve Language Models</div> <div class="author"> Satyapriya Krishna ,&nbsp;Jiaqi Ma ,&nbsp;Dylan Slack ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Sameer Singh ,&nbsp;and&nbsp;Himabindu Lakkaraju </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em> , 2023 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2928_post_hoc_explanations_of_langu.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, lead to critical insights for refining in context learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">krishna2023post</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Post Hoc Explanations of Language Models Can Improve Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Krishna, Satyapriya and Ma, Jiaqi and Slack, Dylan and Ghandeharioun, Asma and Singh, Sameer and Lakkaraju, Himabindu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dissect-480.webp 480w,/assets/img/publication_preview/dissect-800.webp 800w,/assets/img/publication_preview/dissect-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/dissect.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dissect.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2021dissect" class="col-sm-8"> <div class="title">DISSECT: Disentangled simultaneous explanations via concept traversals</div> <div class="author"> <em>Asma Ghandeharioun</em>,&nbsp;Been Kim ,&nbsp;Chun-Liang Li ,&nbsp;Brendan Jou ,&nbsp;Brian Eoff ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR)</em> , 2021 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://mitmedialabaffectivecomputing.github.io/SynthDermDataset/" class="btn btn-sm z-depth-0" role="button">DATA</a> <a href="/assets/pdf/4164_dissect_disentangled_simultane.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/asmadotgh/dissect" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore "what-if" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier’s decision. By training a generative model from a classifier’s signal, DISSECT offers a way to discover a classifier’s inherent "notion" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier’s decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ghandeharioun2021dissect</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DISSECT: Disentangled simultaneous explanations via concept traversals}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghandeharioun, Asma and Kim, Been and Li, Chun-Liang and Jou, Brendan and Eoff, Brian and Picard, Rosalind W}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/thesis-480.webp 480w,/assets/img/publication_preview/thesis-800.webp 800w,/assets/img/publication_preview/thesis-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/thesis.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="thesis.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2021towards" class="col-sm-8"> <div class="title">Towards Human-Centered Optimality Criteria</div> <div class="author"> <em>Asma Ghandeharioun</em> </div> <div class="periodical"> <em>Massachusetts Institute of Technology</em> , 2021 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/ghandeharioun-asma_gh-phd-MAS-2021-thesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Despite the transformational success of machine learning across various applications, examples of deployed models failing to recognize and support human-centered (HC) criteria are abundant. In this thesis, I conceptualize the space of human-machine collaboration with respect to two components: interpretation of people by machines and interpretation of machines by people. I develop several tools that make improvements along these axes. First, I develop a pipeline that predicts depressive symptoms rated by clinicians from real-world longitudinal data outperforming several baselines. Second, I introduce a novel, model-agnostic, and dataset-agnostic method to approximate interactive human evaluation in open-domain dialog through self-play that is more strongly correlated with human evaluations than other automated metrics commonly used today. While dialog quality evaluation metrics predominantly use word-level overlap or distance metrics based on embedding resemblance to each turn of the conversation, I show the significance of taking into account the conversation’s trajectory and using proxies such as sentiment, semantics, and user engagement that are psychologically motivated. Third, I demonstrate an uncertainty measurement technique that helps disambiguate annotator disagreement and data bias. I show that this characterization also improves model performance. Finally, I present a novel method that allows humans to investigate a predictor’s decision-making process to gain better insight into how it works. The method jointly trains a generator, a discriminator, and a concept disentangler, allowing the human to ask "what-if" questions. I evaluate it on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that our method performs consistently well across all. I discuss its applications to detect potential biases of a classifier and identify spurious artifacts that impact predictions using simulated experiments. Together, these novel techniques and insights provide a more comprehensive interpretation of people by machines and more powerful tools for interpretation of machines by people that can move us closer to HC optimality.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">ghandeharioun2021towards</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Human-Centered Optimality Criteria}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghandeharioun, Asma}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Massachusetts Institute of Technology}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mixedeffect-480.webp 480w,/assets/img/publication_preview/mixedeffect-800.webp 800w,/assets/img/publication_preview/mixedeffect-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/mixedeffect.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mixedeffect.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="lewis2023mixed" class="col-sm-8"> <div class="title">Mixed effects random forests for personalised predictions of clinical depression severity</div> <div class="author"> Robert A Lewis ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Szymon Fedor ,&nbsp;Paola Pedrelli ,&nbsp;Rosalind W Picard ,&nbsp;and&nbsp;David Mischoulon </div> <div class="periodical"> <em>In Workshop on Computational Approaches to Mental Health @ ICML</em> , 2021 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://icml.cc/virtual/2021/13391" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="/assets/pdf/2301.09815.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This work demonstrates how mixed effects random forests enable accurate predictions of depression severity using multimodal physiological and digital activity data collected from an 8-week study involving 31 patients with major depressive disorder. We show that mixed effects random forests outperform standard random forests and personal average baselines when predicting clinical Hamilton Depression Rating Scale scores (HDRS_17). Compared to the latter baseline, accuracy is significantly improved for each patient by an average of 0.199-0.276 in terms of mean absolute error (p&lt;0.05). This is noteworthy as these simple baselines frequently outperform machine learning methods in mental health prediction tasks. We suggest that this improved performance results from the ability of the mixed effects random forest to personalise model parameters to individuals in the dataset. However, we find that these improvements pertain exclusively to scenarios where labelled patient data are available to the model at training time. Investigating methods that improve accuracy when generalising to new patients is left as important future work.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dialog-480.webp 480w,/assets/img/publication_preview/dialog-800.webp 800w,/assets/img/publication_preview/dialog-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/dialog.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dialog.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="jaques2020human" class="col-sm-8"> <div class="title">Human-centric dialog training via offline reinforcement learning</div> <div class="author"> Natasha Jaques* ,&nbsp;Judy Hanwen Shen* ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Craig Ferguson ,&nbsp;Agata Lapedriza ,&nbsp;Noah Jones ,&nbsp;Shixiang Shane Gu ,&nbsp;and&nbsp;Rosalind Picard </div> <div class="periodical"> <em>In Empirical Methods in Natural Language Processing (EMNLP)</em> , 2020 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> (Oral) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2020.emnlp-main.327.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/natashamjaques/neural_chat" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning (RL). We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions. A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward. These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions. We solve the challenge by developing a novel class of offline RL algorithms. These algorithms use KL-control to penalize divergence from a pre-trained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty. We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches. The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jaques2020human</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Human-centric dialog training via offline reinforcement learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jaques*, Natasha and Shen*, Judy Hanwen and Ghandeharioun, Asma and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang Shane and Picard, Rosalind}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Empirical Methods in Natural Language Processing (EMNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dialog-hierarchical-480.webp 480w,/assets/img/publication_preview/dialog-hierarchical-800.webp 800w,/assets/img/publication_preview/dialog-hierarchical-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/dialog-hierarchical.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dialog-hierarchical.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="saleh2020hierarchical" class="col-sm-8"> <div class="title">Hierarchical reinforcement learning for open-domain dialog</div> <div class="author"> Abdelrhman Saleh* ,&nbsp;Natasha Jaques* ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Judy Shen ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>In Proceedings of the AAAI conference on artificial intelligence (AAAI)</em> , 2020 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> (Oral) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/6400-Article%20Text-9625-1-10-20200517.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/natashamjaques/neural_chat" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Open-domain dialog generation is a challenging problem; maximum likelihood training can lead to repetitive outputs, models have difficulty tracking long-term conversational goals, and training on standard movie or online datasets may lead to the generation of inappropriate, biased, or offensive text. Reinforcement Learning (RL) is a powerful framework that could potentially address these issues, for example by allowing a dialog model to optimize for reducing toxicity and repetitiveness. However, previous approaches which apply RL to open-domain dialog generation do so at the word level, making it difficult for the model to learn proper credit assignment for long-term conversational rewards. In this paper, we propose a novel approach to hierarchical reinforcement learning, VHRL, which uses policy gradients to tune the utterance-level embedding of a variational sequence model. This hierarchical approach provides greater flexibility for learning long-term, conversational rewards. We use self-play and RL to optimize for a set of human-centered conversation metrics, and show that our approach provides significant improvements – in terms of both human evaluation and automatic metrics – over state-of-the-art dialog models, including Transformers.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">saleh2020hierarchical</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hierarchical reinforcement learning for open-domain dialog}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Saleh*, Abdelrhman and Jaques*, Natasha and Ghandeharioun, Asma and Shen, Judy and Picard, Rosalind W}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI conference on artificial intelligence (AAAI)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{05}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8741--8748}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/justintime-480.webp 480w,/assets/img/publication_preview/justintime-800.webp 800w,/assets/img/publication_preview/justintime-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/justintime.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="justintime.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="zepf2020studying" class="col-sm-8"> <div class="title">Studying personalized just-in-time auditory breathing guides and potential safety implications during simulated driving</div> <div class="author"> Sebastian Zepf ,&nbsp;Neska El Haouij ,&nbsp;Jinmo Lee ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Javier Hernandez ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>In Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization</em> , 2020 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3340631.3394854" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="/assets/pdf/3340631.3394854.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Driving can occupy a considerable part of our daily lives and is often associated with high levels of stress. Motivated by the effectiveness of controlled breathing, this work studies the potential use of breathing interventions while driving to help manage stress. In particular, we implemented and evaluated a closed-loop system that monitored the breathing rate of drivers in real-time and delivered either a conscious or an unconscious personalized acoustic breathing guide whenever needed. In a study with 24 participants, we observed that conscious interventions more effectively reduced the breathing rate but also increased the number of driving mistakes. We observed that prior driving experience as well as personality are significantly associated with the effect of the interventions, which highlights the importance of considering user profiles for in-car stress management interventions.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/depression-480.webp 480w,/assets/img/publication_preview/depression-800.webp 800w,/assets/img/publication_preview/depression-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/depression.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="depression.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="pedrelli2020monitoring" class="col-sm-8"> <div class="title">Monitoring changes in depression severity using wearable and mobile sensors</div> <div class="author"> Paola Pedrelli ,&nbsp;Szymon Fedor ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Esther Howe ,&nbsp;Dawn F Ionescu ,&nbsp;Darian Bhathena ,&nbsp;Lauren B Fisher ,&nbsp;Cristina Cusin ,&nbsp;Maren Nyer ,&nbsp;Albert Yeung ,&nbsp;and&nbsp; others </div> <div class="periodical"> <em>Frontiers in psychiatry</em>, 2020 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2020.584711/full" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="/assets/pdf/fpsyt-11-584711.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Background: While preliminary evidence suggests that sensors may be employed to detect presence of low mood it is still unclear whether they can be leveraged for measuring depression symptom severity. This study evaluates the feasibility and performance of assessing depressive symptom severity by using behavioral and physiological features obtained from wristband and smartphone sensors. Method: Participants were thirty-one individuals with Major Depressive Disorder (MDD). The protocol included 8 weeks of behavioral and physiological monitoring through smartphone and wristband sensors and six in-person clinical interviews during which depression was assessed with the 17-item Hamilton Depression Rating Scale (HDRS-17). Results: Participants wore the right and left wrist sensors 92 and 94% of the time respectively. Three machine-learning models estimating depressive symptom severity were developed–one combining features from smartphone and wearable sensors, one including only features from the smartphones, and one including features from wrist sensors–and evaluated in two different scenarios. Correlations between the models’ estimate of HDRS scores and clinician-rated HDRS ranged from moderate to high (0.46 [CI: 0.42, 0.74] to 0.7 [CI: 0.66, 0.74]) and had moderate accuracy with Mean Absolute Error ranging between 3.88 ± 0.18 and 4.74 ± 1.24. The time-split scenario of the model including only features from the smartphones performed the best. The ten most predictive features in the model combining physiological and mobile features were related to mobile phone engagement, activity level, skin conductance, and heart rate variability. Conclusion: Monitoring of MDD patients through smartphones and wrist sensors following a clinician-rated HDRS assessment is feasible and may provide an estimate of changes in depressive symptom severity. Future studies should further examine the best features to estimate depressive symptoms and strategies to further enhance accuracy.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/correlations_table-480.webp 480w,/assets/img/publication_preview/correlations_table-800.webp 800w,/assets/img/publication_preview/correlations_table-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/correlations_table.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="correlations_table.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2019approximating" class="col-sm-8"> <div class="title">Approximating interactive human evaluation with self-play for open-domain dialog systems</div> <div class="author"> <em>Asma Ghandeharioun*</em>,&nbsp;Judy Hanwen Shen* ,&nbsp;Natasha Jaques* ,&nbsp;Craig Ferguson ,&nbsp;Noah Jones ,&nbsp;Agata Lapedriza ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em> , 2019 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/NeurIPS-2019-approximating-interactive-human-evaluation-with-self-play-for-open-domain-dialog-systems-Paper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/natashamjaques/neural_chat" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Building an open-domain conversational agent is a challenging problem. Current evaluation methods, mostly post-hoc judgments of static conversation, do not capture conversation quality in a realistic interactive context. In this paper, we investigate interactive human evaluation and provide evidence for its necessity; we then introduce a novel, model-agnostic, and dataset-agnostic method to approximate it. In particular, we propose a self-play scenario where the dialog system talks to itself and we calculate a combination of proxies such as sentiment and semantic coherence on the conversation trajectory. We show that this metric is capable of capturing the human-rated quality of a dialog model better than any automated metric known to-date, achieving a significant Pearson correlation (r&gt;.7, p&lt;.05). To investigate the strengths of this novel metric and interactive evaluation in comparison to state-of-the-art metrics and human evaluation of static conversations, we perform extended experiments with a set of models, including several that make novel improvements to recent hierarchical dialog generation architectures through sentiment and semantic knowledge distillation on the utterance level. Finally, we open-source the interactive evaluation platform we built and the dataset we collected to allow researchers to efficiently deploy and evaluate dialog models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ghandeharioun2019approximating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Approximating interactive human evaluation with self-play for open-domain dialog systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghandeharioun*, Asma and Shen*, Judy Hanwen and Jaques*, Natasha and Ferguson, Craig and Jones, Noah and Lapedriza, Agata and Picard, Rosalind W}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/hrl-480.webp 480w,/assets/img/publication_preview/hrl-800.webp 800w,/assets/img/publication_preview/hrl-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/hrl.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hrl.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="saleh2020hierarchicam" class="col-sm-8"> <div class="title">Hierarchical reinforcement learning for open-domain dialog</div> <div class="author"> Abdelrhman Saleh* ,&nbsp;Natasha Jaques* ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Judy Shen ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>In Conversational AI (NeurIPS-W)</em> , 2019 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> (Best paper nominee) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/6400-Article%20Text-9625-1-10-20200517.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/natashamjaques/neural_chat" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Open-domain dialog generation is a challenging problem; maximum likelihood training can lead to repetitive outputs, models have difficulty tracking long-term conversational goals, and training on standard movie or online datasets may lead to the generation of inappropriate, biased, or offensive text. Reinforcement Learning (RL) is a powerful framework that could potentially address these issues, for example by allowing a dialog model to optimize for reducing toxicity and repetitiveness. However, previous approaches which apply RL to open-domain dialog generation do so at the word level, making it difficult for the model to learn proper credit assignment for long-term conversational rewards. In this paper, we propose a novel approach to hierarchical reinforcement learning, VHRL, which uses policy gradients to tune the utterance-level embedding of a variational sequence model. This hierarchical approach provides greater flexibility for learning long-term, conversational rewards. We use self-play and RL to optimize for a set of human-centered conversation metrics, and show that our approach provides significant improvements – in terms of both human evaluation and automatic metrics – over state-of-the-art dialog models, including Transformers.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">saleh2020hierarchicam</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hierarchical reinforcement learning for open-domain dialog}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Saleh*, Abdelrhman and Jaques*, Natasha and Ghandeharioun, Asma and Shen, Judy and Picard, Rosalind W}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conversational AI (NeurIPS-W)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/brl_grid_example_pb-480.webp 480w,/assets/img/publication_preview/brl_grid_example_pb-800.webp 800w,/assets/img/publication_preview/brl_grid_example_pb-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/brl_grid_example_pb.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="brl_grid_example_pb.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="jaques2019way" class="col-sm-8"> <div class="title">Way off-policy batch deep reinforcement learning of implicit human preferences in dialog</div> <div class="author"> Natasha Jaques ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Judy Hanwen Shen ,&nbsp;Craig Ferguson ,&nbsp;Agata Lapedriza ,&nbsp;Noah Jones ,&nbsp;Shixiang Gu ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>Conversational AI (NeurIPS-W)</em>, 2019 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1907.00456" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/natashamjaques/neural_chat" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Most deep reinforcement learning (RL) systems are not able to learn effectively from off-policy data, especially if they cannot explore online in the environment. These are critical shortcomings for applying RL to real-world problems where collecting data is expensive, and models must be tested offline before being deployed to interact with the environment – e.g. systems that learn from human interaction. Thus, we develop a novel class of off-policy batch RL algorithms, which are able to effectively learn offline, without exploring, from a fixed batch of human interaction data. We leverage models pre-trained on data as a strong prior, and use KL-control to penalize divergence from this prior during RL training. We also use dropout-based uncertainty estimates to lower bound the target Q-values as a more efficient alternative to Double Q-Learning. The algorithms are tested on the problem of open-domain dialog generation – a challenging reinforcement learning problem with a 20,000-dimensional action space. Using our Way Off-Policy algorithm, we can extract multiple different reward functions post-hoc from collected human interaction data, and learn effectively from all of these. We test the real-world generalization of these systems by deploying them live to converse with humans in an open-domain setting, and demonstrate that our algorithm achieves significant improvements over prior methods in off-policy batch RL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">jaques2019way</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Way off-policy batch deep reinforcement learning of implicit human preferences in dialog}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind W}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Conversational AI (NeurIPS-W)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/uncnet-480.webp 480w,/assets/img/publication_preview/uncnet-800.webp 800w,/assets/img/publication_preview/uncnet-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/uncnet.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="uncnet.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2019characterizing" class="col-sm-8"> <div class="title">Characterizing sources of uncertainty to proxy calibration and disambiguate annotator and data bias</div> <div class="author"> <em>Asma Ghandeharioun</em>,&nbsp;Brian Eoff ,&nbsp;Brendan Jou ,&nbsp;and&nbsp;Rosalind Picard </div> <div class="periodical"> <em>In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCV-W)</em> , 2019 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> (Oral) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/1909.09285.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/asmadotgh/unc-net" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Supporting model interpretability for complex phenomena where annotators can legitimately disagree, such as emotion recognition, is a challenging machine learning task. In this work, we show that explicitly quantifying the uncertainty in such settings has interpretability benefits. We use a simple modification of a classical network inference using Monte Carlo dropout to give measures of epistemic and aleatoric uncertainty. We identify a significant correlation between aleatoric uncertainty and human annotator disagreement (r≈.3). Additionally, we demonstrate how difficult and subjective training samples can be identified using aleatoric uncertainty and how epistemic uncertainty can reveal data bias that could result in unfair predictions. We identify the total uncertainty as a suitable surrogate for model calibration, i.e. the degree we can trust model’s predicted confidence. In addition to explainability benefits, we observe modest performance boosts from incorporating model uncertainty.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ghandeharioun2019characterizing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Characterizing sources of uncertainty to proxy calibration and disambiguate annotator and data bias}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghandeharioun, Asma and Eoff, Brian and Jou, Brendan and Picard, Rosalind}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCV-W)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4202--4206}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/emma-480.webp 480w,/assets/img/publication_preview/emma-800.webp 800w,/assets/img/publication_preview/emma-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/emma.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="emma.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2019emma" class="col-sm-8"> <div class="title">EMMA: An Emotion-Aware Wellbeing Chatbot</div> <div class="author"> <em>Asma Ghandeharioun</em>,&nbsp;Daniel McDuff ,&nbsp;Mary Czerwinski ,&nbsp;and&nbsp;Kael Rowan </div> <div class="periodical"> <em>In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)</em> , 2019 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/ACII_10.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The delivery of mental health interventions via ubiquitous devices has shown much promise. A conversational chatbot is a promising oracle for delivering appropriate just-in-time interventions. However, designing emotionally-aware agents, specially in this context, is under-explored. Furthermore, the feasibility of automating the delivery of just-in-time mHealth interventions via such an agent has not been fully studied. In this paper, we present the design and evaluation of EMMA (EMotion-Aware mHealth Agent) through a two-week long human-subject experiment with N=39 participants. EMMA provides emotionally appropriate micro-activities in an empathetic manner. We show that the system can be extended to detect a user’s mood purely from smartphone sensor data. Our results show that our personalized machine learning model was perceived as likable via self-reports of emotion from users. Finally, we provide a set of guidelines for the design of emotion-aware bots for mHealth.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ghandeharioun2019emma</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{EMMA: An Emotion-Aware Wellbeing Chatbot}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghandeharioun, Asma and McDuff, Daniel and Czerwinski, Mary and Rowan, Kael}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/UI-480.webp 480w,/assets/img/publication_preview/UI-800.webp 800w,/assets/img/publication_preview/UI-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/UI.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="UI.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2019towards" class="col-sm-8"> <div class="title">Towards understanding emotional intelligence for behavior change chatbots</div> <div class="author"> <em>Asma Ghandeharioun</em>,&nbsp;Daniel McDuff ,&nbsp;Mary Czerwinski ,&nbsp;and&nbsp;Kael Rowan </div> <div class="periodical"> <em>In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)</em> , 2019 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/ACII_9.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>A natural conversational interface that allows longitudinal symptom tracking would be extremely valuable in health/wellness applications. However, the task of designing emotionally-aware agents for behavior change is still poorly understood. In this paper, we present the design and evaluation of an emotion-aware chatbot that conducts experience sampling in an empathetic manner. We evaluate it through a human-subject experiment with N=39 participants over the course of a week. Our results show that extraverts preferred the emotion-aware chatbot significantly more than introverts. Also, participants reported a higher percentage of positive mood reports when interacting with the empathetic bot. Finally, we provide guidelines for the design of emotion-aware chatbots for potential use in mHealth contexts.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ghandeharioun2019towards</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards understanding emotional intelligence for behavior change chatbots}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghandeharioun, Asma and McDuff, Daniel and Czerwinski, Mary and Rowan, Kael}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8--14}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/brainbeat-480.webp 480w,/assets/img/publication_preview/brainbeat-800.webp 800w,/assets/img/publication_preview/brainbeat-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/brainbeat.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="brainbeat.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="leslie2019engineering" class="col-sm-8"> <div class="title">Engineering music to slow breathing and invite relaxed physiology</div> <div class="author"> Grace Leslie ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Diane Zhou ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>In 2019 8th international conference on Affective Computing and Intelligent Interaction (ACII)</em> , 2019 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.computer.org/csdl/proceedings-article/acii/2019/08925531/1fHGEL5fRde" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="/assets/pdf/ACII_117.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We engineered an interactive music system that influences a user’s breathing rate to induce a relaxation response. This system generates ambient music containing periodic shifts in loudness that are determined by the user’s own breathing patterns. We evaluated the efficacy of this music intervention for participants who were engaged in an attention-demanding task, and thus explicitly not focusing on their breathing or on listening to the music. We measured breathing patterns in addition to multiple peripheral and cortical indicators of physiological arousal while users experienced three different interaction designs: (1) a “Fixed Tempo” amplitude modulation rate at six beats per minute; (2) a “Personalized Tempo” modulation rate fixed at 75% of each individual’s breathing rate baseline, and (3) a “Personalized Envelope” design in which the amplitude modulation matches each individual’s breathing pattern in real-time. Our results revealed that each interactive music design slowed down breathing rates, with the “Personalized Tempo” design having the largest effect, one that was more significant than the non-personalized design. The physiological arousal indicators (electrodermal activity, heart rate, and slow cortical potentials measured in EEG) showed concomitant reductions, suggesting that slowing users’ breathing rates shifted them towards a more calmed state. These results suggest that interactive music incorporating biometric data may have greater effects on physiology than traditional recorded music. </p> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/suicidepred-480.webp 480w,/assets/img/publication_preview/suicidepred-800.webp 800w,/assets/img/publication_preview/suicidepred-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/suicidepred.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="suicidepred.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="jones2019analysis" class="col-sm-8"> <div class="title">Analysis of online suicide risk with document embeddings and latent dirichlet allocation</div> <div class="author"> Noah Jones ,&nbsp;Natasha Jaques ,&nbsp;Pat Pataranutaporn ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>In 2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)</em> , 2019 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/8925077" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>Machine learning to infer suicide risk and urgency is applied to a dataset of Reddit users in which the risk and urgency labels were derived from crowdsource consensus. We present the results of machine learning models based on transfer learning from document embeddings trained on large external corpora, and find that they have very high F1 scores (.83 -. 92) in distinguishing which users are labeled as being most at risk of committing suicide. We further show that the document embedding approach outperforms a method based on word importance, where important words were identified by domain experts. Finally, we find, using a Latent Dirichlet Allocation (LDA) topic model, that users labeled at-risk for suicide post about different topics to the rest of Reddit than non-suicidal users.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/depressionbdi-480.webp 480w,/assets/img/publication_preview/depressionbdi-800.webp 800w,/assets/img/publication_preview/depressionbdi-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/depressionbdi.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="depressionbdi.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="howe2018depression" class="col-sm-8"> <div class="title">Depression and emotional reactivity: a closer examination of daily variations in affect</div> <div class="author"> Esther Howe ,&nbsp;Maya Nauphal ,&nbsp;Ben Shapero ,&nbsp;Kate Bentley ,&nbsp;David Mischoulon ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Szymon Fedor ,&nbsp;Rosalind W Picard ,&nbsp;and&nbsp;Paola Pedrelli </div> <div class="periodical"> <em>Association for Behavioral and Cognitive Therapies Annual Convention (ABCT)</em>, 2018 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/brightbeat-480.webp 480w,/assets/img/publication_preview/brightbeat-800.webp 800w,/assets/img/publication_preview/brightbeat-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/brightbeat.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="brightbeat.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2017brightbeat" class="col-sm-8"> <div class="title">BrightBeat: Effortlessly influencing breathing for cultivating calmness and focus</div> <div class="author"> <em>Asma Ghandeharioun</em>,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>In Proceedings of the 2017 CHI conference extended abstracts on human factors in computing systems</em> , 2017 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/3027063.3053164" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>While technology is usually associated with causing stress, technology also has the potential to bring about calm. In particular, breathing usually speeds up with higher stress, but it can be slowed through a manipulation, and in so doing, it can help the person lower their stress and improve their focus. This paper introduces BrightBeat, a set of seamless visual and auditory interventions that look like respiratory biofeedback, rhythmically oscillating, but that are tuned to appear with a slower speed, with the aim of slowing a stressed computer user’s breathing and, consequently, bringing a sense of focus and calmness. These interventions were designed to run easily on commonplace personal electronic devices and to not require any focused attention in order to be effective. We have run a randomized placebo-controlled trial and examined both objective and subjective measures of impact with N=32 users undergoing work tasks. BrightBeat significantly influenced slower breathing, had a lasting effect, improved self-reported calmness and focus, and was highly preferred for future use.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/objectivedepression-480.webp 480w,/assets/img/publication_preview/objectivedepression-800.webp 800w,/assets/img/publication_preview/objectivedepression-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/objectivedepression.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="objectivedepression.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2017objective" class="col-sm-8"> <div class="title">Objective Assessment of Depressive Symptoms with Machine Learning and Wearable Sensors Data</div> <div class="author"> <em>Asma Ghandeharioun</em>,&nbsp;Szymon Fedor ,&nbsp;Lisa Sangermano ,&nbsp;Dawn F Ionescu ,&nbsp;Jonathan Alpert ,&nbsp;Chelsea Dale ,&nbsp;David Sontag ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>In 2017 7th international conference on Affective Computing and Intelligent Interaction (ACII)</em> , 2017 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/8273620" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="/assets/pdf/17.ghandeharioun_etal_objective_ACII.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Depression is the major cause of years lived in disability world-wide; however, its diagnosis and tracking methods still rely mainly on assessing self-reported depressive symptoms, methods that originated more than fifty years ago. These methods, which usually involve filling out surveys or engaging in face-to-face interviews, provide limited accuracy and reliability and are costly to track and scale. In this paper, we develop and test the efficacy of machine learning techniques applied to objective data captured passively and continuously from E4 wearable wristbands and from sensors in an Android phone for predicting the Hamilton Depression Rating Scale (HDRS). Input data include electrodermal activity (EDA), sleep behavior, motion, phone-based communication, location changes, and phone usage patterns. We introduce our feature generation and transformation process, imputing missing clinical scores from self-reported measures, and predicting depression severity from continuous sensor measurements. While HDRS ranges between 0 and 52, we were able to impute it with 2.8 RMSE and predict it with 4.5 RMSE which are low relative errors. Analyzing the features and their relation to depressive symptoms, we found that poor mental health was accompanied by more irregular sleep, less motion, fewer incoming messages, less variability in location patterns, and higher asymmetry of EDA between the right and the left wrists.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mghdepression-480.webp 480w,/assets/img/publication_preview/mghdepression-800.webp 800w,/assets/img/publication_preview/mghdepression-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/mghdepression.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mghdepression.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="pedrelli2017integrating" class="col-sm-8"> <div class="title">Integrating EMA, Clinical Assessment and Wearable Sensors to Examine the Association between Major Depressive Disorder (MDD) and Alcohol Use</div> <div class="author"> Paola Pedrelli ,&nbsp;Esther Howe ,&nbsp;David Mischoulon ,&nbsp;Rosalind W Picard ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;and&nbsp;Szymon Fedor </div> <div class="periodical"> <em>Connected Health Conference</em>, 2017 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.iproc.org/2017/1/e51/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>Background: Depression is the leading cause of disability worldwide. Heavy drinking often co-occurs with Major Depressive Disorder (MDD), preventing the amelioration of symptoms and increasing disability. New technology-based assessment tools such as ecological momentary assessment (EMA) and wearable sensors provide the opportunity for a more detailed examination of the interplay between these two conditions. While the association between low mood and heavy drinking has been extensively examined, multi-method assessments including EMA, sensors and clinician-rated measures have not been utilized to study the association between depression and heavy alcohol use. Objective: To examine the association between depressive symptoms and alcohol consumption by integrating multiple sources of data including EMA, clinical assessment, and wearable sensors. Methods: Individuals with MDD complete an 8-week protocol that involves tracking depressive symptoms and alcohol consumption daily through an EMA. Mood is captured via surveys delivered twice daily that include 10 items of the Positive and Negative Affect Scale (PANAS) assessing negative affect (NA) and positive affect (PA). Participants wear Empatica E4 wristband sensors that track electrodermal activity (EDA) and accelerometer data 23 hours/day. The clinician-rated Hamilton Depression Rating Scale (HDRS) is administered biweekly to assess depressive symptoms. MovisensXS, the app delivering the EMA, tracks text messages, phone calls, location, app usage, and screen on/off behavior. Results: To date, 16 of 30 projected participants have completed the study. All participants are expected to complete the study by 10/2017. Preliminary analyses confirmed the accuracy of the daily mood ratings. There was a significant linear relationship between NA/PA ratio from EMA ratings and the clinician-based ratings (P=1.3e-6). To focus on the association of low mood and drinking behavior, we solely included instances where NA&gt;=PA and observed a significant association (P=0.001) between low mood (as a ratio of total NA divided by PA) and higher alcohol use. Analyses will be repeated for all 30 participants. E4 accelerometer data and location data will help elucidate whether mobility moderates the association between mood and depression, such that individuals who drink at home may exhibit greater depressive symptomatology. Finally, the association between EDA and alcohol use will be examined. Final results will be presented at the Connected Health Conference. Conclusions: To date, results show a significant association between low mood and alcohol consumption. Results of planned analyses will further clarify the temporal association between mood and alcohol use among depressed patients, and possible moderators and mediators of this relationship. A precise understanding of the association between low mood, physiological states and heavy drinking will facilitate the development of “just-in-time” ecological momentary interventions for both reduction of depressed mood and heavy drinking.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/objectivevsubjective-480.webp 480w,/assets/img/publication_preview/objectivevsubjective-800.webp 800w,/assets/img/publication_preview/objectivevsubjective-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/objectivevsubjective.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="objectivevsubjective.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2017objectivf" class="col-sm-8"> <div class="title">Objective vs. Subjective Reports of Sleep Quality in Major Depressive Disorder: A Pilot Study</div> <div class="author"> <em>Asma Ghandeharioun</em>,&nbsp;Lisa Sangermano ,&nbsp;Rosalind W Picard ,&nbsp;Jonathan Alpert ,&nbsp;Chelsea Dale ,&nbsp;Dawn F Ionescu ,&nbsp;and&nbsp;Szymon Fedor </div> <div class="periodical"> <em>Anxiety and Depression Association of America (ADAA)</em>, 2017 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Background: The diagnosis of major depressive disorder (MDD) is heterogeneous. For example, depressed patients exhibit varied patterns of sleep; both insomnia and hypersomnia are symptoms of depression. Assessment of sleep patterns in MDD is often limited by clinicians’ reliance on subjective self-ratings of sleep quality. Objective measures, such as sleep regularity measured by accelerometer data, may provide a more accurate understanding. This study assessed the extent to which objective sleep quality measures could detect differences among individuals with MDD. We hypothesized that we would observe variability in sleep regularity and patterns among depressed individuals. We also hypothesized that there would be a strong correlation between subjective sleep ratings and objective measurements. Methods: Between April and October 2016, patients with MDD (n=4) and healthy volunteers (n=2) completed a protocol that involved tracking depressive symptoms and wearing Empatica E4 wristbands that recorded accelerometer data. Patients wore sensors 23 hours daily and were clinically assessed for depression symptoms and stress levels biweekly for 8 weeks using the Hamilton Depression Rating Scale (HDRS). We developed an algorithm to calculate objective sleep based on accelerometer data. We calculated sleep regularity indices for both objective and subjective sleep. We utilized Pearson correlation to compare sleep regularity indices and t-statistics to compare the sleep regularity between depressed and healthy samples. Results: On average, the objective (accelerometer-based) and subjective (self-reported) sleep/awake time periods matched 74.52% of the time for each user (std=6.55%). A trend toward positive correlation between objective and subjective sleep regularity indices did not reach statistical significance in this small sample (r=0.79, p=0.06). Another trend that suggested depressed users had a lower objective sleep regularity index was also not significant (p=0.14). Conclusion: Our analyses revealed that individuals’ subjective sleep ratings and objective data from the E4 sensors were potentially correlated but did not reach significance in this sample. Further analyses also suggested an association between lower objective sleep regularity and depressive symptoms as measured by the HDRS. We are continuing to recruit new patients for this protocol and further data will be reported from the larger sample during the conference.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cellphone-480.webp 480w,/assets/img/publication_preview/cellphone-800.webp 800w,/assets/img/publication_preview/cellphone-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/cellphone.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cellphone.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="sangermano2017incoming" class="col-sm-8"> <div class="title">Cell phone data as a potential predictor of depression severity: a pilot study</div> <div class="author"> Lisa Sangermano ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Rosalind W Picard ,&nbsp;Jonathan Alpert ,&nbsp;Chelsea Dale ,&nbsp;Szymon Fedor ,&nbsp;and&nbsp;Dawn F Ionescu </div> <div class="periodical"> <em>Anxiety and Depression Association of America (ADAA)</em>, 2017 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Background: Major depressive disorder (MDD) is a serious and prevalent disease with an unpredictable course. Cell phone technology can assist doctors by monitoring patients’ symptoms, and may eventually be useful in the prediction of depressive episode time courses. However, the extent to which the course of depression can be predicted with cell phone data remains unknown. The quantitative measurement of communication patterns (i.e., number of text messages and phone calls) between depressed patients and their contacts may be useful for the prognostication of the course of depression. We predicted that more communication with social contacts via texts and phone calls would correlate with lower depression scores. Methods: Between April 2016 and March 2017, patients with MDD (n=12) and healthy volunteers (n=4) completed an 8-week protocol that involved tracking depressive symptoms and mobile phone usage. All patients were assessed at 2-week intervals for depression symptoms as measured with the Hamilton Depression Rating Scale (HDRS). Movisens (an Android application) was used to measure incoming and outgoing SMS (text messages) and phone calls, and missed calls. We used linear mixed-effects models with random intercepts and slopes to assess the relationship between the HDRS total score and the number of calls/texts in the week prior to clinical assessment. Results: An increased duration of calls received during the week prior to clinical assessment were related to increases in total HDRS scores (t=0.005, p=0.012). However, an increased number of calls made during the week prior to the assessment were related to decreases in total HDRS scores (t=-0.887, p=0.033). Conclusion: Contrary to our hypothesis, depressed patients with longer incoming calls in the week prior to their study visit had significantly more subjectively reported symptoms of depression. Though the content of interactions was unknown, this may represent a relationship between negative interactions with social contacts and increased depressive symptoms. Alternatively, as patients’ social contacts begin to suspect worsening depression, they may mobilize support through cell phone communications. Further research is necessary to discern the context of social connectedness and symptoms of depression. On the other hand, the HDRS symptoms improved as the depressed individual made more calls the week prior to the assessment. This could be interpreted as the social interactions that are made at will can improve the depressive symptoms.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/loc-480.webp 480w,/assets/img/publication_preview/loc-800.webp 800w,/assets/img/publication_preview/loc-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/loc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="loc.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="howe2017location" class="col-sm-8"> <div class="title">Location patterns from phone sensors may help predict depressive symptoms: a longitudinal pilot study</div> <div class="author"> Esther Howe ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Paola Pedrelli ,&nbsp;David Mischoulon ,&nbsp;Rosalind W Picard ,&nbsp;and&nbsp;Szymon Fedor </div> <div class="periodical"> <em>Association for Behavioral and Cognitive Therapies Annual Convention –Tech SIG (ABCT)</em>, 2017 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Major depressive disorder (MDD) is a serious and prevalent disease with a variable course. Ubiquitous smartphone technology has the potential to inform and improve clinical care by monitoring patients’ symptoms and behavioral patterns.The extent to which the changes in depressive symptoms can be predicted with cell phone data remains unknown.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mghdepressionicons-480.webp 480w,/assets/img/publication_preview/mghdepressionicons-800.webp 800w,/assets/img/publication_preview/mghdepressionicons-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/mghdepressionicons.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mghdepressionicons.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2017location" class="col-sm-8"> <div class="title">Location variability from commodity phone sensors is negatively associated with self-reported depression score: a pilot study</div> <div class="author"> <em>Asma Ghandeharioun</em>,&nbsp;Szymon Fedor ,&nbsp;Lisa Sangermano ,&nbsp;Jonathan Alpert ,&nbsp;Chelsea Dale ,&nbsp;Dawn F Ionescu ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>Association for Psychological Science (APS)</em>, 2017 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/kindandgrateful-480.webp 480w,/assets/img/publication_preview/kindandgrateful-800.webp 800w,/assets/img/publication_preview/kindandgrateful-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/kindandgrateful.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kindandgrateful.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2016kind" class="col-sm-8"> <div class="title">"Kind and Grateful": A context-sensitive smartphone app utilizing inspirational content to promote gratitude</div> <div class="author"> <em>Asma Ghandeharioun</em>,&nbsp;Asaph Azaria ,&nbsp;Sara Taylor ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>Psychology of well-being</em>, 2016 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://psywb.springeropen.com/articles/10.1186/s13612-016-0046-2" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="/assets/pdf/s13612-016-0046-2.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Background: Previous research has shown that gratitude positively influences psychological wellbeing and physical health. Grateful people are reported to feel more optimistic and happy, to better mitigate aversive experiences, and to have stronger interpersonal bonds. Gratitude interventions have been shown to result in improved sleep, more frequent exercise and stronger cardiovascular and immune systems. These findings call for the development of technologies that would inspire gratitude. This paper presents a novel system designed toward this end. Methods: We leverage pervasive technologies to naturally embed inspiration to express gratitude in everyday life. Novel to this work, mobile sensor data is utilized to infer optimal moments for stimulating contextually relevant thankfulness and appreciation. Sporadic mood measurements are inventively obtained through the smartphone lock screen, investigating their interplay with grateful expressions. Both momentary thankful emotion and dispositional gratitude are measured. To evaluate our system, we ran two rounds of randomized control trials (RCT), including a pilot study (N = 15, 2 weeks) and a main study (N = 27, 5 weeks). Studies’ participants were provided with a newly developed smartphone app through which they were asked to express gratitude; the app displayed inspirational content to only the intervention group, while measuring contextual cues for all users. Results: In both rounds of the RCT, the intervention was associated with improved thankful behavior. Significant increase was observed in multiple facets of practicing gratitude in the intervention groups. The average frequency of practicing thankfulness increased by more than 120 %, comparing the baseline weeks with the intervention weeks of the main study. In contrast, the control group of the same study exhibited a decrease of 90 % in the frequency of thankful expressions. In the course of the study’s 5 weeks, increases in dispositional gratitude and in psychological wellbeing were also apparent. Analyzing the relation between mood and gratitude expressions, our data suggest that practicing gratitude increases the probability of going up in terms of emotional valence and down in terms of emotional arousal. The influences of inspirational content and contextual cues on promoting thankful behavior were also analyzed: We present data suggesting that the more successful times for eliciting expressions of gratitude tend to be shortly after a social experience, shortly after location change, and shortly after physical activity. Conclusions: The results support our intervention as an impactful method to promote grateful affect and behavior. Moreover, they provide insights into design and evaluation of general behavioral intervention technologies.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sleepmood-480.webp 480w,/assets/img/publication_preview/sleepmood-800.webp 800w,/assets/img/publication_preview/sleepmood-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/sleepmood.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sleepmood.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="taylor2016machine" class="col-sm-8"> <div class="title">Machine learning of sleep and wake behaviors to classify self-reported evening mood</div> <div class="author"> Sara Taylor ,&nbsp;Natasha Jaques ,&nbsp;Akane Sano ,&nbsp;Asaph Azaria ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>SLEEP</em>, 2016 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The SNAPSHOT Study is a large-scale and long-term study that seeks to measure: Sleep, Networks, Affect, Performance, Stress, and Health using Objective Techniques. This study investigates: (1) how daily behaviors influence sleep, stress, mood, and other wellbeing-related factors (2) how accurately we can recognize/predict stress, mood and wellbeing (3) how interactions in a social network influence sleep behaviors. In this work we investigate the use of machine learning methods, using sleep and wake data, to predict mood. We seek to model behavioral patterns to predict these downturns in mood and begin to understand what will help build resilience to depression. Our results reveal that stress and happiness can be predicted most reliably from these signals, and that data collected while participants were asleep is particularly important to classifying happiness.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/13612_2016_46_Fig6-480.webp 480w,/assets/img/publication_preview/13612_2016_46_Fig6-800.webp 800w,/assets/img/publication_preview/13612_2016_46_Fig6-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/13612_2016_46_Fig6.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="13612_2016_46_Fig6.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="ghandeharioun2016promoting" class="col-sm-8"> <div class="title">Promoting kindness and gratitude with a smartphone and triggers</div> <div class="author"> <em>Asma Ghandeharioun</em>,&nbsp;Asaph Azaria ,&nbsp;Sara Taylor ,&nbsp;Pattie Maes ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>Annals of Behavioral Medicine</em>, 2016 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/moodpred-480.webp 480w,/assets/img/publication_preview/moodpred-800.webp 800w,/assets/img/publication_preview/moodpred-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/moodpred.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="moodpred.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="jaques2015predicting" class="col-sm-8"> <div class="title">Predicting students’ happiness from physiology, phone, mobility, and behavioral data</div> <div class="author"> Natasha Jaques ,&nbsp;Sara Taylor ,&nbsp;Asaph Azaria ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Akane Sano ,&nbsp;and&nbsp;Rosalind W Picard </div> <div class="periodical"> <em>In 2015 International Conference on Affective Computing and Intelligent Interaction (ACII)</em> , 2015 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/7344575" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="/assets/pdf/10-ACII.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In order to model students’ happiness, we apply machine learning methods to data collected from undergrad students monitored over the course of one month each. The data collected include physiological signals, location, smartphone logs, and survey responses to behavioral questions. Each day, participants reported their wellbeing on measures including stress, health, and happiness. Because of the relationship between happiness and depression, modeling happiness may help us to detect individuals who are at risk of depression and guide interventions to help them. We are also interested in how behavioral factors (such as sleep and social activity) affect happiness positively and negatively. A variety of machine learning and feature selection techniques are compared, including Gaussian Mixture Models and ensemble classification. We achieve 70% classification accuracy of self-reported happiness on held-out test data.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2013</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="alizadehsani2013data" class="col-sm-8"> <div class="title">A data mining approach for diagnosis of coronary artery disease</div> <div class="author"> Roohallah Alizadehsani ,&nbsp;Jafar Habibi ,&nbsp;Mohammad Javad Hosseini ,&nbsp;Hoda Mashayekhi ,&nbsp;Reihane Boghrati ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Behdad Bahadorian ,&nbsp;and&nbsp;Zahra Alizadeh Sani </div> <div class="periodical"> <em>Computer methods and programs in biomedicine</em>, 2013 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://pubmed.ncbi.nlm.nih.gov/23537611/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">alizadehsani2013data</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A data mining approach for diagnosis of coronary artery disease}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alizadehsani, Roohallah and Habibi, Jafar and Hosseini, Mohammad Javad and Mashayekhi, Hoda and Boghrati, Reihane and Ghandeharioun, Asma and Bahadorian, Behdad and Sani, Zahra Alizadeh}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer methods and programs in biomedicine}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{111}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{52--61}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="alizadehsani2013diagnosing" class="col-sm-8"> <div class="title">Diagnosing coronary artery disease via data mining algorithms by considering laboratory and echocardiography features</div> <div class="author"> Roohallah Alizadehsani ,&nbsp;Jafar Habibi ,&nbsp;Zahra Alizadeh Sani ,&nbsp;Hoda Mashayekhi ,&nbsp;Reihane Boghrati ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Fahime Khozeimeh ,&nbsp;and&nbsp;Fariba Alizadeh-Sani </div> <div class="periodical"> <em>Research in cardiovascular medicine</em>, 2013 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4253773/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2012</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="alizadehsani2012diagnosis" class="col-sm-8"> <div class="title">Diagnosis of coronary arteries stenosis using data mining</div> <div class="author"> Roohallah Alizadehsani ,&nbsp;Jafar Habibi ,&nbsp;Behdad Bahadorian ,&nbsp;Hoda Mashayekhi ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Reihane Boghrati ,&nbsp;and&nbsp;Zahra Alizadeh Sani </div> <div class="periodical"> <em>Journal of Medical Signals and Sensors</em>, 2012 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://pubmed.ncbi.nlm.nih.gov/23717807/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">alizadehsani2012diagnosis</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Diagnosis of coronary arteries stenosis using data mining}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alizadehsani, Roohallah and Habibi, Jafar and Bahadorian, Behdad and Mashayekhi, Hoda and Ghandeharioun, Asma and Boghrati, Reihane and Sani, Zahra Alizadeh}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Medical Signals and Sensors}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{153--159}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Medknow}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="alizadehsani2012diagnosit" class="col-sm-8"> <div class="title">Diagnosis of coronary artery disease using cost-sensitive algorithms</div> <div class="author"> Roohallah Alizadehsani ,&nbsp;Mohammad Javad Hosseini ,&nbsp;Zahra Alizadeh Sani ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;and&nbsp;Reihane Boghrati </div> <div class="periodical"> <em>In 2012 IEEE 12th International Conference on Data Mining Workshops (ICDM)</em> , 2012 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a href="https://ieeexplore.ieee.org/document/6406417" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="alizadehsani2012diagnosiu" class="col-sm-8"> <div class="title">Diagnosis of coronary artery disease using data mining based on lab data and echo features</div> <div class="author"> Roohallah Alizadehsani ,&nbsp;Jafar Habibi ,&nbsp;Zahra Alizadeh Sani ,&nbsp;Hoda Mashayekhi ,&nbsp;Reihane Boghrati ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;and&nbsp;Behdad Bahadorian </div> <div class="periodical"> <em>Journal of Medical and Bioengineering</em>, 2012 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> <a href="/assets/pdf/20130412053847102.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="alizadehsani2012exerting" class="col-sm-8"> <div class="title">Exerting cost-sensitive and feature creation algorithms for coronary artery disease diagnosis</div> <div class="author"> Roohallah Alizadehsani ,&nbsp;Mohammad Javad Hosseini ,&nbsp;Reihane Boghrati ,&nbsp;<em>Asma Ghandeharioun</em>,&nbsp;Fahime Khozeimeh ,&nbsp;and&nbsp;Zahra Alizadeh Sani </div> <div class="periodical"> <em>International Journal of Knowledge Discovery in Bioinformatics (IJKDB)</em>, 2012 </div> <div class="periodical"> </div> <div class="periodical" style="color: #FF1D66; font-weight: bold; font-style: italic;"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> &copy; Copyright 2024 Asma Ghandeharioun. Last updated: July 11, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-FNVJMQM77L"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-FNVJMQM77L");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>